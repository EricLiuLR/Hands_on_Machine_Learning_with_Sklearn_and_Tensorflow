{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenges for deep neuralnets:\n",
    "- vanishing gradients and exploding gradients problem\n",
    "- slow training\n",
    "- overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients often get smaller and smaller during the backpropagation process. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the opposite can happen(mostly in recurrent neural networks): the gradients get bigger and bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using logistiv sigmoid activation function and random initialization using a strandard normal distribution, the variance of the outpu of each layer is much larger than its input.\n",
    "When the inputs get larger, the sigmoid function saturates at 0 or 1, with a derivatie close to 0, this will cause the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the signal to flow properly in both directions: forward for prediction and backward for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the output of each layer to be euqal to its input, and we also need the gradient to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Xavier and He Initialization** can do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these initialization strategies can speed up the training considerably, and it is one of the tricks that led to the current success of Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And also, using Nonsaturating Activation Functions can help**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, ReLU behave much better than sigmoid function, mostly because it does not saturate for positive values, and also because it is quite fast to compute.\n",
    "\n",
    "However, it suffers from a problem called \"*Dying ReLUS*\": during training, some neurons effectively die, meaning they stop outputting anything other than 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this, you can use \"***Leaky ReLU***\" or \"***exponential linear unit(ELU)***\" instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which activation function to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generally ELU > leaky ReLU > ReLU > tanh > logistic\n",
    "- if you care much about runtime performance, then leaky ReLu over ELU\n",
    "- if you don't want to tweak another hyperparameter, use alpha=0.01 for leaky ReLU and alpha=1 for ELU as the default\n",
    "- however, if you have enough time, use cv to determine all these stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Besides, Batch Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization consists of adding an operation in the model just before the activation function of each layer, simply zero-centerting and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean if the inputs for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique makes the model very robust against vanishing gradients problem and makes it possible to use much larger learning rate to speed up training.\n",
    "\n",
    "However, it makes it much slower to test. So it you need prediction to be lightning-fast, you may want to check ELU + He initialization before rushing into Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this nerwork: This is called transfer learning.\n",
    "\n",
    "It will not only speed up training considerably, but will also require much less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Transfer Learning will work only well if the inputs have similar low-level features (can be achieved by preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we can speed up the training process by:\n",
    "- good ininitialization\n",
    "- good activation function\n",
    "- batch normalization\n",
    "- reuse pretrain models\n",
    "\n",
    "Here comes another: **Faster Optimizers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spolier alert: The conclusion of this is that you should almose always use Adam optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is used as acceletation, not speed.\n",
    "\n",
    "Faster, but might overshood a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the gradient not at the local position, but a little ahead in the direction of the momentum\n",
    "\n",
    "Even Faster, closer to the optimum, but still, overshoot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use adaptive learning rate, performs well for simple quadratic problems, but unfortunately often stops too early when training neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the early stopping problem in AdaGrad, already a good-enough optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam(*adaptive moment estimation*) combines the ideas of momentum and RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping works well in practice, but you can often get a better performance by combining it with other regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l1 and l2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The most popular regularization techniques for deep neural networks is arguably dropout.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every training step, every neuron (including the input neurons but excluding the output neurons) has a probability p(typically 50%) of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next training step. After training, neurons don't get dropped out anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: during test, each neuron will be taking much more inputs than it does in training, so the weights must be adjusted.\n",
    "\n",
    "In general, we need to multiply each input connection weight by the *keep peobability*(1-p) after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neuron, it constrains the weights if the incoming connections to be less than gamma.\n",
    "\n",
    "Another regularization technique that is quite popular for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating new training instances from existing ones, boosting the size of the training set.\n",
    "\n",
    "Generating realistic training instances; ideally a human should not be able to tell which instances was generated and which was not.\n",
    "\n",
    "Simply adding white noise could not help, the modifications should be learnable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting should works well in most cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialization: He initialization\n",
    "- Activation Function: ELU\n",
    "- Normalization: Batch Normalization\n",
    "- Regularization: Dropout\n",
    "- Optimizer: Adam\n",
    "- Learning rate schedule: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some other advice:\n",
    "- if you can't find a good learning rate, add an exponential decay learning rate schedule might help\n",
    "- if you training set is a bit too small, try data augmentation\n",
    "- if you need a sparse model, use l1 regularization\n",
    "- if you want a lightning-fast model, drop Barch Normalization, maybe repalce ELU with leaky ReLU, having a sparse model will also help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
